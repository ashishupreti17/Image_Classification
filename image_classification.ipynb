{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2564b0d8208>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function \n",
    "    a = 0\n",
    "    b = 255\n",
    "    return (x-a)/(b-a)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    return lb.transform(x) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    \n",
    "    return tf.placeholder(tf.float32, shape = [None] + list(image_shape),name = 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, n_classes], name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    return  tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    \n",
    "    #print('xtensor: {} , conv_num_outputs : {} \\n conv_ksize: {} \\n conv_stride: {} \\n pool_size : {} \\n '\\\n",
    "    #     'pool_strides: {}'.format(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides))\n",
    "                 \n",
    "    shape = x_tensor.shape.as_list()\n",
    "    \n",
    "    #print ('shape: {}'.format(shape))\n",
    "    #intialize weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0], shape[3], conv_num_outputs],0,0.1)),\n",
    "    bias  = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    # add layer\n",
    "    conv_layer = tf.nn.conv2d(x_tensor,weights, strides = [1] + list(conv_strides) + [1], padding = 'SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias) # add bias\n",
    "    \n",
    "    # add relu activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    #add max pooling\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "                 conv_layer, \n",
    "                 ksize = [1] + list(pool_ksize) + [1] ,\n",
    "                 strides= [1] + list(pool_strides) + [1] ,\n",
    "                 padding= 'SAME')\n",
    "   \n",
    "    return  conv_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from numpy import prod\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    return tf.reshape(x_tensor, [-1, prod(x_tensor.shape.as_list()[1:])])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    shape = x_tensor.shape.as_list()\n",
    "    \n",
    "    shape = list( (shape[1],) + (num_outputs,))\n",
    "    \n",
    "    #print(shape)\n",
    "    \n",
    "    #intialize weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal(shape,0,0.1))\n",
    "    bias  = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    # add layer\n",
    "    fc_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    \n",
    "    # add relu activation function\n",
    "    fc_layer = tf.nn.relu(fc_layer)\n",
    "    \n",
    "    return fc_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    \n",
    "    shape = x_tensor.shape.as_list()\n",
    "    shape = list( (shape[1],) + (num_outputs,))\n",
    "    \n",
    "    # initialize weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal(shape,0,0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    #add layer\n",
    "    out_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    \n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    #conv_layer = conv2d_maxpool(x,conv_num_outputs =8,conv_ksize =(8,8),conv_strides=(2,2),pool_ksize =(4,4),pool_strides=(4,4))\n",
    "    #conv_layer = conv2d_maxpool(x,conv_num_outputs=16,conv_ksize =(4,4),conv_strides=(2,2),pool_ksize =(4,4),pool_strides=(2,2))\n",
    "    #conv_layer = conv2d_maxpool(x,conv_num_outputs=18,conv_ksize =(4,4),conv_strides=(1,1),pool_ksize =(8,8),pool_strides=(4,4))\n",
    "    #conv_layer = conv2d_maxpool(x,conv_num_outputs=16,conv_ksize =(4,4),conv_strides=(1,1),pool_ksize =(4,4),pool_strides=(1,1))\n",
    "    conv_layer = conv2d_maxpool(x,conv_num_outputs=18,conv_ksize =(3,3),conv_strides=(1,1),pool_ksize =(2,2),pool_strides=(2,2))\n",
    "    #conv_layer = conv2d_maxpool(x,conv_num_outputs =8,conv_ksize =(2,2),conv_strides=(2,2),pool_ksize =(2,2),pool_strides=(2,2))\n",
    "    \n",
    "    conv_layer = tf.nn.dropout(conv_layer,keep_prob)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flatten_layer = flatten(conv_layer)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    #fully_conn_layer = fully_conn(flatten_layer, 10)\n",
    "    #fully_conn_layer = fully_conn(flatten_layer, 20)\n",
    "    fully_conn_layer = fully_conn(flatten_layer, 200)\n",
    "    \n",
    "    fully_conn_layer = tf.nn.dropout(fully_conn_layer, keep_prob)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out_layer = output(fully_conn_layer, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch,y: label_batch,keep_prob: 1.0})\n",
    "    valid_accuracy = session.run(accuracy,feed_dict={x: valid_features,y: valid_labels,keep_prob: 1.0})\n",
    "    \n",
    "    print(\"loss  : {} and Validation : {} \".format(loss, valid_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss  : 2.2173476219177246 and Validation : 0.20759999752044678 \n",
      "Epoch  2, CIFAR-10 Batch 1:  loss  : 2.1188316345214844 and Validation : 0.2770000100135803 \n",
      "Epoch  3, CIFAR-10 Batch 1:  loss  : 2.0373282432556152 and Validation : 0.3301999866962433 \n",
      "Epoch  4, CIFAR-10 Batch 1:  loss  : 1.9736406803131104 and Validation : 0.35760000348091125 \n",
      "Epoch  5, CIFAR-10 Batch 1:  loss  : 1.8839406967163086 and Validation : 0.3864000141620636 \n",
      "Epoch  6, CIFAR-10 Batch 1:  loss  : 1.8252103328704834 and Validation : 0.4007999897003174 \n",
      "Epoch  7, CIFAR-10 Batch 1:  loss  : 1.7677600383758545 and Validation : 0.41819998621940613 \n",
      "Epoch  8, CIFAR-10 Batch 1:  loss  : 1.7302067279815674 and Validation : 0.4359999895095825 \n",
      "Epoch  9, CIFAR-10 Batch 1:  loss  : 1.6880420446395874 and Validation : 0.43459999561309814 \n",
      "Epoch 10, CIFAR-10 Batch 1:  loss  : 1.6448529958724976 and Validation : 0.44020000100135803 \n",
      "Epoch 11, CIFAR-10 Batch 1:  loss  : 1.6148414611816406 and Validation : 0.45559999346733093 \n",
      "Epoch 12, CIFAR-10 Batch 1:  loss  : 1.5647509098052979 and Validation : 0.4546000063419342 \n",
      "Epoch 13, CIFAR-10 Batch 1:  loss  : 1.538262128829956 and Validation : 0.4578000009059906 \n",
      "Epoch 14, CIFAR-10 Batch 1:  loss  : 1.4873340129852295 and Validation : 0.4717999994754791 \n",
      "Epoch 15, CIFAR-10 Batch 1:  loss  : 1.475954294204712 and Validation : 0.4733999967575073 \n",
      "Epoch 16, CIFAR-10 Batch 1:  loss  : 1.4556580781936646 and Validation : 0.46880000829696655 \n",
      "Epoch 17, CIFAR-10 Batch 1:  loss  : 1.4512813091278076 and Validation : 0.4803999960422516 \n",
      "Epoch 18, CIFAR-10 Batch 1:  loss  : 1.401127576828003 and Validation : 0.4832000136375427 \n",
      "Epoch 19, CIFAR-10 Batch 1:  loss  : 1.3732048273086548 and Validation : 0.4844000041484833 \n",
      "Epoch 20, CIFAR-10 Batch 1:  loss  : 1.3410143852233887 and Validation : 0.4880000054836273 \n",
      "Epoch 21, CIFAR-10 Batch 1:  loss  : 1.3528578281402588 and Validation : 0.48500001430511475 \n",
      "Epoch 22, CIFAR-10 Batch 1:  loss  : 1.2962496280670166 and Validation : 0.4952000081539154 \n",
      "Epoch 23, CIFAR-10 Batch 1:  loss  : 1.2943305969238281 and Validation : 0.49799999594688416 \n",
      "Epoch 24, CIFAR-10 Batch 1:  loss  : 1.2300671339035034 and Validation : 0.4991999864578247 \n",
      "Epoch 25, CIFAR-10 Batch 1:  loss  : 1.2243483066558838 and Validation : 0.4986000061035156 \n",
      "Epoch 26, CIFAR-10 Batch 1:  loss  : 1.2236659526824951 and Validation : 0.4986000061035156 \n",
      "Epoch 27, CIFAR-10 Batch 1:  loss  : 1.1912211179733276 and Validation : 0.5012000203132629 \n",
      "Epoch 28, CIFAR-10 Batch 1:  loss  : 1.1659806966781616 and Validation : 0.5072000026702881 \n",
      "Epoch 29, CIFAR-10 Batch 1:  loss  : 1.177994728088379 and Validation : 0.5008000135421753 \n",
      "Epoch 30, CIFAR-10 Batch 1:  loss  : 1.1458017826080322 and Validation : 0.5108000040054321 \n",
      "Epoch 31, CIFAR-10 Batch 1:  loss  : 1.1135658025741577 and Validation : 0.5157999992370605 \n",
      "Epoch 32, CIFAR-10 Batch 1:  loss  : 1.1083042621612549 and Validation : 0.5131999850273132 \n",
      "Epoch 33, CIFAR-10 Batch 1:  loss  : 1.0877140760421753 and Validation : 0.5167999863624573 \n",
      "Epoch 34, CIFAR-10 Batch 1:  loss  : 1.0550451278686523 and Validation : 0.5153999924659729 \n",
      "Epoch 35, CIFAR-10 Batch 1:  loss  : 1.0699644088745117 and Validation : 0.5188000202178955 \n",
      "Epoch 36, CIFAR-10 Batch 1:  loss  : 1.0264675617218018 and Validation : 0.5188000202178955 \n",
      "Epoch 37, CIFAR-10 Batch 1:  loss  : 1.0168416500091553 and Validation : 0.517799973487854 \n",
      "Epoch 38, CIFAR-10 Batch 1:  loss  : 0.9995132684707642 and Validation : 0.5220000147819519 \n",
      "Epoch 39, CIFAR-10 Batch 1:  loss  : 0.9782397150993347 and Validation : 0.5242000222206116 \n",
      "Epoch 40, CIFAR-10 Batch 1:  loss  : 0.956436038017273 and Validation : 0.5203999876976013 \n",
      "Epoch 41, CIFAR-10 Batch 1:  loss  : 0.9550802111625671 and Validation : 0.524399995803833 \n",
      "Epoch 42, CIFAR-10 Batch 1:  loss  : 0.9479658007621765 and Validation : 0.5228000283241272 \n",
      "Epoch 43, CIFAR-10 Batch 1:  loss  : 0.910921573638916 and Validation : 0.5249999761581421 \n",
      "Epoch 44, CIFAR-10 Batch 1:  loss  : 0.9394370913505554 and Validation : 0.5230000019073486 \n",
      "Epoch 45, CIFAR-10 Batch 1:  loss  : 0.9115358591079712 and Validation : 0.5230000019073486 \n",
      "Epoch 46, CIFAR-10 Batch 1:  loss  : 0.8876330256462097 and Validation : 0.5246000289916992 \n",
      "Epoch 47, CIFAR-10 Batch 1:  loss  : 0.8755337595939636 and Validation : 0.5270000100135803 \n",
      "Epoch 48, CIFAR-10 Batch 1:  loss  : 0.8735416531562805 and Validation : 0.5288000106811523 \n",
      "Epoch 49, CIFAR-10 Batch 1:  loss  : 0.8565797805786133 and Validation : 0.527999997138977 \n",
      "Epoch 50, CIFAR-10 Batch 1:  loss  : 0.8819414377212524 and Validation : 0.5299999713897705 \n",
      "Epoch 51, CIFAR-10 Batch 1:  loss  : 0.8614708185195923 and Validation : 0.5289999842643738 \n",
      "Epoch 52, CIFAR-10 Batch 1:  loss  : 0.8301782608032227 and Validation : 0.5353999733924866 \n",
      "Epoch 53, CIFAR-10 Batch 1:  loss  : 0.8228687047958374 and Validation : 0.5343999862670898 \n",
      "Epoch 54, CIFAR-10 Batch 1:  loss  : 0.8182374238967896 and Validation : 0.5325999855995178 \n",
      "Epoch 55, CIFAR-10 Batch 1:  loss  : 0.801694393157959 and Validation : 0.5378000140190125 \n",
      "Epoch 56, CIFAR-10 Batch 1:  loss  : 0.7932693362236023 and Validation : 0.5356000065803528 \n",
      "Epoch 57, CIFAR-10 Batch 1:  loss  : 0.8054212331771851 and Validation : 0.5307999849319458 \n",
      "Epoch 58, CIFAR-10 Batch 1:  loss  : 0.7719067335128784 and Validation : 0.5392000079154968 \n",
      "Epoch 59, CIFAR-10 Batch 1:  loss  : 0.7758753895759583 and Validation : 0.5357999801635742 \n",
      "Epoch 60, CIFAR-10 Batch 1:  loss  : 0.8020811080932617 and Validation : 0.5296000242233276 \n",
      "Epoch 61, CIFAR-10 Batch 1:  loss  : 0.7497838735580444 and Validation : 0.531000018119812 \n",
      "Epoch 62, CIFAR-10 Batch 1:  loss  : 0.7646087408065796 and Validation : 0.5415999889373779 \n",
      "Epoch 63, CIFAR-10 Batch 1:  loss  : 0.7815331816673279 and Validation : 0.5296000242233276 \n",
      "Epoch 64, CIFAR-10 Batch 1:  loss  : 0.7346619963645935 and Validation : 0.5368000268936157 \n",
      "Epoch 65, CIFAR-10 Batch 1:  loss  : 0.7198464274406433 and Validation : 0.5414000153541565 \n",
      "Epoch 66, CIFAR-10 Batch 1:  loss  : 0.712576150894165 and Validation : 0.5392000079154968 \n",
      "Epoch 67, CIFAR-10 Batch 1:  loss  : 0.7080764174461365 and Validation : 0.5473999977111816 \n",
      "Epoch 68, CIFAR-10 Batch 1:  loss  : 0.6987751722335815 and Validation : 0.5351999998092651 \n",
      "Epoch 69, CIFAR-10 Batch 1:  loss  : 0.7055165767669678 and Validation : 0.5375999808311462 \n",
      "Epoch 70, CIFAR-10 Batch 1:  loss  : 0.6810380816459656 and Validation : 0.5350000262260437 \n",
      "Epoch 71, CIFAR-10 Batch 1:  loss  : 0.6701964139938354 and Validation : 0.5436000227928162 \n",
      "Epoch 72, CIFAR-10 Batch 1:  loss  : 0.6852328181266785 and Validation : 0.5407999753952026 \n",
      "Epoch 73, CIFAR-10 Batch 1:  loss  : 0.691798985004425 and Validation : 0.5406000018119812 \n",
      "Epoch 74, CIFAR-10 Batch 1:  loss  : 0.6683025360107422 and Validation : 0.5473999977111816 \n",
      "Epoch 75, CIFAR-10 Batch 1:  loss  : 0.643671452999115 and Validation : 0.5465999841690063 \n",
      "Epoch 76, CIFAR-10 Batch 1:  loss  : 0.6387137770652771 and Validation : 0.5432000160217285 \n",
      "Epoch 77, CIFAR-10 Batch 1:  loss  : 0.6121876835823059 and Validation : 0.546999990940094 \n",
      "Epoch 78, CIFAR-10 Batch 1:  loss  : 0.6171250343322754 and Validation : 0.5437999963760376 \n",
      "Epoch 79, CIFAR-10 Batch 1:  loss  : 0.6552823185920715 and Validation : 0.5379999876022339 \n",
      "Epoch 80, CIFAR-10 Batch 1:  loss  : 0.623672366142273 and Validation : 0.5386000275611877 \n",
      "Epoch 81, CIFAR-10 Batch 1:  loss  : 0.622020423412323 and Validation : 0.5473999977111816 \n",
      "Epoch 82, CIFAR-10 Batch 1:  loss  : 0.5900678634643555 and Validation : 0.5386000275611877 \n",
      "Epoch 83, CIFAR-10 Batch 1:  loss  : 0.596206545829773 and Validation : 0.5493999719619751 \n",
      "Epoch 84, CIFAR-10 Batch 1:  loss  : 0.5808693170547485 and Validation : 0.551800012588501 \n",
      "Epoch 85, CIFAR-10 Batch 1:  loss  : 0.5662523508071899 and Validation : 0.5508000254631042 \n",
      "Epoch 86, CIFAR-10 Batch 1:  loss  : 0.571951687335968 and Validation : 0.5461999773979187 \n",
      "Epoch 87, CIFAR-10 Batch 1:  loss  : 0.5729436874389648 and Validation : 0.5461999773979187 \n",
      "Epoch 88, CIFAR-10 Batch 1:  loss  : 0.5511516332626343 and Validation : 0.5519999861717224 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, CIFAR-10 Batch 1:  loss  : 0.5562577843666077 and Validation : 0.5446000099182129 \n",
      "Epoch 90, CIFAR-10 Batch 1:  loss  : 0.5452896356582642 and Validation : 0.5496000051498413 \n",
      "Epoch 91, CIFAR-10 Batch 1:  loss  : 0.5775150060653687 and Validation : 0.550000011920929 \n",
      "Epoch 92, CIFAR-10 Batch 1:  loss  : 0.5289202332496643 and Validation : 0.5464000105857849 \n",
      "Epoch 93, CIFAR-10 Batch 1:  loss  : 0.5718146562576294 and Validation : 0.5379999876022339 \n",
      "Epoch 94, CIFAR-10 Batch 1:  loss  : 0.5521723031997681 and Validation : 0.5440000295639038 \n",
      "Epoch 95, CIFAR-10 Batch 1:  loss  : 0.5269012451171875 and Validation : 0.5479999780654907 \n",
      "Epoch 96, CIFAR-10 Batch 1:  loss  : 0.5284664034843445 and Validation : 0.5375999808311462 \n",
      "Epoch 97, CIFAR-10 Batch 1:  loss  : 0.502751350402832 and Validation : 0.5504000186920166 \n",
      "Epoch 98, CIFAR-10 Batch 1:  loss  : 0.5185758471488953 and Validation : 0.5490000247955322 \n",
      "Epoch 99, CIFAR-10 Batch 1:  loss  : 0.5089560151100159 and Validation : 0.5465999841690063 \n",
      "Epoch 100, CIFAR-10 Batch 1:  loss  : 0.4978378415107727 and Validation : 0.5514000058174133 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss  : 2.2457001209259033 and Validation : 0.19179999828338623 \n",
      "Epoch  1, CIFAR-10 Batch 2:  loss  : 2.0541980266571045 and Validation : 0.2786000072956085 \n",
      "Epoch  1, CIFAR-10 Batch 3:  loss  : 1.861907958984375 and Validation : 0.335999995470047 \n",
      "Epoch  1, CIFAR-10 Batch 4:  loss  : 1.8154582977294922 and Validation : 0.3779999911785126 \n",
      "Epoch  1, CIFAR-10 Batch 5:  loss  : 1.8013486862182617 and Validation : 0.3946000039577484 \n",
      "Epoch  2, CIFAR-10 Batch 1:  loss  : 1.7868249416351318 and Validation : 0.41839998960494995 \n",
      "Epoch  2, CIFAR-10 Batch 2:  loss  : 1.6841014623641968 and Validation : 0.4311999976634979 \n",
      "Epoch  2, CIFAR-10 Batch 3:  loss  : 1.5108474493026733 and Validation : 0.4390000104904175 \n",
      "Epoch  2, CIFAR-10 Batch 4:  loss  : 1.5442078113555908 and Validation : 0.44519999623298645 \n",
      "Epoch  2, CIFAR-10 Batch 5:  loss  : 1.6516189575195312 and Validation : 0.46000000834465027 \n",
      "Epoch  3, CIFAR-10 Batch 1:  loss  : 1.665832757949829 and Validation : 0.4702000021934509 \n",
      "Epoch  3, CIFAR-10 Batch 2:  loss  : 1.5437567234039307 and Validation : 0.4733999967575073 \n",
      "Epoch  3, CIFAR-10 Batch 3:  loss  : 1.3877030611038208 and Validation : 0.4772000014781952 \n",
      "Epoch  3, CIFAR-10 Batch 4:  loss  : 1.436903715133667 and Validation : 0.4848000109195709 \n",
      "Epoch  3, CIFAR-10 Batch 5:  loss  : 1.5878489017486572 and Validation : 0.47940000891685486 \n",
      "Epoch  4, CIFAR-10 Batch 1:  loss  : 1.6136791706085205 and Validation : 0.4941999912261963 \n",
      "Epoch  4, CIFAR-10 Batch 2:  loss  : 1.431820273399353 and Validation : 0.4869999885559082 \n",
      "Epoch  4, CIFAR-10 Batch 3:  loss  : 1.3279507160186768 and Validation : 0.48840001225471497 \n",
      "Epoch  4, CIFAR-10 Batch 4:  loss  : 1.3416829109191895 and Validation : 0.5001999735832214 \n",
      "Epoch  4, CIFAR-10 Batch 5:  loss  : 1.5421996116638184 and Validation : 0.4943999946117401 \n",
      "Epoch  5, CIFAR-10 Batch 1:  loss  : 1.5321696996688843 and Validation : 0.5062000155448914 \n",
      "Epoch  5, CIFAR-10 Batch 2:  loss  : 1.3793632984161377 and Validation : 0.498199999332428 \n",
      "Epoch  5, CIFAR-10 Batch 3:  loss  : 1.2882236242294312 and Validation : 0.5022000074386597 \n",
      "Epoch  5, CIFAR-10 Batch 4:  loss  : 1.2939059734344482 and Validation : 0.5126000046730042 \n",
      "Epoch  5, CIFAR-10 Batch 5:  loss  : 1.4663201570510864 and Validation : 0.5121999979019165 \n",
      "Epoch  6, CIFAR-10 Batch 1:  loss  : 1.5138072967529297 and Validation : 0.5163999795913696 \n",
      "Epoch  6, CIFAR-10 Batch 2:  loss  : 1.3208069801330566 and Validation : 0.5139999985694885 \n",
      "Epoch  6, CIFAR-10 Batch 3:  loss  : 1.269990086555481 and Validation : 0.5108000040054321 \n",
      "Epoch  6, CIFAR-10 Batch 4:  loss  : 1.2588064670562744 and Validation : 0.5206000208854675 \n",
      "Epoch  6, CIFAR-10 Batch 5:  loss  : 1.4539530277252197 and Validation : 0.5163999795913696 \n",
      "Epoch  7, CIFAR-10 Batch 1:  loss  : 1.4547500610351562 and Validation : 0.5194000005722046 \n",
      "Epoch  7, CIFAR-10 Batch 2:  loss  : 1.3042341470718384 and Validation : 0.5157999992370605 \n",
      "Epoch  7, CIFAR-10 Batch 3:  loss  : 1.2248518466949463 and Validation : 0.515999972820282 \n",
      "Epoch  7, CIFAR-10 Batch 4:  loss  : 1.2195295095443726 and Validation : 0.5249999761581421 \n",
      "Epoch  7, CIFAR-10 Batch 5:  loss  : 1.4101793766021729 and Validation : 0.5231999754905701 \n",
      "Epoch  8, CIFAR-10 Batch 1:  loss  : 1.4159531593322754 and Validation : 0.5278000235557556 \n",
      "Epoch  8, CIFAR-10 Batch 2:  loss  : 1.2131435871124268 and Validation : 0.5246000289916992 \n",
      "Epoch  8, CIFAR-10 Batch 3:  loss  : 1.2028124332427979 and Validation : 0.5299999713897705 \n",
      "Epoch  8, CIFAR-10 Batch 4:  loss  : 1.1680290699005127 and Validation : 0.5333999991416931 \n",
      "Epoch  8, CIFAR-10 Batch 5:  loss  : 1.3904176950454712 and Validation : 0.5329999923706055 \n",
      "Epoch  9, CIFAR-10 Batch 1:  loss  : 1.3495779037475586 and Validation : 0.5386000275611877 \n",
      "Epoch  9, CIFAR-10 Batch 2:  loss  : 1.168858528137207 and Validation : 0.5332000255584717 \n",
      "Epoch  9, CIFAR-10 Batch 3:  loss  : 1.1314146518707275 and Validation : 0.5335999727249146 \n",
      "Epoch  9, CIFAR-10 Batch 4:  loss  : 1.1595125198364258 and Validation : 0.534600019454956 \n",
      "Epoch  9, CIFAR-10 Batch 5:  loss  : 1.3834190368652344 and Validation : 0.5397999882698059 \n",
      "Epoch 10, CIFAR-10 Batch 1:  loss  : 1.351738691329956 and Validation : 0.5404000282287598 \n",
      "Epoch 10, CIFAR-10 Batch 2:  loss  : 1.157705307006836 and Validation : 0.5461999773979187 \n",
      "Epoch 10, CIFAR-10 Batch 3:  loss  : 1.0948281288146973 and Validation : 0.54339998960495 \n",
      "Epoch 10, CIFAR-10 Batch 4:  loss  : 1.120557188987732 and Validation : 0.5410000085830688 \n",
      "Epoch 10, CIFAR-10 Batch 5:  loss  : 1.3484742641448975 and Validation : 0.5440000295639038 \n",
      "Epoch 11, CIFAR-10 Batch 1:  loss  : 1.321202278137207 and Validation : 0.5440000295639038 \n",
      "Epoch 11, CIFAR-10 Batch 2:  loss  : 1.1648741960525513 and Validation : 0.5425999760627747 \n",
      "Epoch 11, CIFAR-10 Batch 3:  loss  : 1.1177915334701538 and Validation : 0.5378000140190125 \n",
      "Epoch 11, CIFAR-10 Batch 4:  loss  : 1.0961135625839233 and Validation : 0.5455999970436096 \n",
      "Epoch 11, CIFAR-10 Batch 5:  loss  : 1.3325674533843994 and Validation : 0.5436000227928162 \n",
      "Epoch 12, CIFAR-10 Batch 1:  loss  : 1.3185474872589111 and Validation : 0.5515999794006348 \n",
      "Epoch 12, CIFAR-10 Batch 2:  loss  : 1.1478625535964966 and Validation : 0.550000011920929 \n",
      "Epoch 12, CIFAR-10 Batch 3:  loss  : 1.0687834024429321 and Validation : 0.5483999848365784 \n",
      "Epoch 12, CIFAR-10 Batch 4:  loss  : 1.0723040103912354 and Validation : 0.5515999794006348 \n",
      "Epoch 12, CIFAR-10 Batch 5:  loss  : 1.312067985534668 and Validation : 0.5527999997138977 \n",
      "Epoch 13, CIFAR-10 Batch 1:  loss  : 1.286831259727478 and Validation : 0.5526000261306763 \n",
      "Epoch 13, CIFAR-10 Batch 2:  loss  : 1.0992103815078735 and Validation : 0.5532000064849854 \n",
      "Epoch 13, CIFAR-10 Batch 3:  loss  : 1.0602816343307495 and Validation : 0.5419999957084656 \n",
      "Epoch 13, CIFAR-10 Batch 4:  loss  : 1.0585901737213135 and Validation : 0.553600013256073 \n",
      "Epoch 13, CIFAR-10 Batch 5:  loss  : 1.295016884803772 and Validation : 0.5497999787330627 \n",
      "Epoch 14, CIFAR-10 Batch 1:  loss  : 1.2524477243423462 and Validation : 0.5555999875068665 \n",
      "Epoch 14, CIFAR-10 Batch 2:  loss  : 1.1127777099609375 and Validation : 0.5558000206947327 \n",
      "Epoch 14, CIFAR-10 Batch 3:  loss  : 1.0367836952209473 and Validation : 0.5551999807357788 \n",
      "Epoch 14, CIFAR-10 Batch 4:  loss  : 1.0369036197662354 and Validation : 0.5631999969482422 \n",
      "Epoch 14, CIFAR-10 Batch 5:  loss  : 1.2897841930389404 and Validation : 0.5591999888420105 \n",
      "Epoch 15, CIFAR-10 Batch 1:  loss  : 1.2454049587249756 and Validation : 0.5562000274658203 \n",
      "Epoch 15, CIFAR-10 Batch 2:  loss  : 1.070800542831421 and Validation : 0.5605999827384949 \n",
      "Epoch 15, CIFAR-10 Batch 3:  loss  : 1.0367577075958252 and Validation : 0.5604000091552734 \n",
      "Epoch 15, CIFAR-10 Batch 4:  loss  : 1.0090696811676025 and Validation : 0.5633999705314636 \n",
      "Epoch 15, CIFAR-10 Batch 5:  loss  : 1.2673377990722656 and Validation : 0.5631999969482422 \n",
      "Epoch 16, CIFAR-10 Batch 1:  loss  : 1.1934585571289062 and Validation : 0.5631999969482422 \n",
      "Epoch 16, CIFAR-10 Batch 2:  loss  : 1.0447978973388672 and Validation : 0.5598000288009644 \n",
      "Epoch 16, CIFAR-10 Batch 3:  loss  : 1.055402398109436 and Validation : 0.5651999711990356 \n",
      "Epoch 16, CIFAR-10 Batch 4:  loss  : 0.9865673184394836 and Validation : 0.5619999766349792 \n",
      "Epoch 16, CIFAR-10 Batch 5:  loss  : 1.2620923519134521 and Validation : 0.5658000111579895 \n",
      "Epoch 17, CIFAR-10 Batch 1:  loss  : 1.1779083013534546 and Validation : 0.5626000165939331 \n",
      "Epoch 17, CIFAR-10 Batch 2:  loss  : 1.045023798942566 and Validation : 0.5690000057220459 \n",
      "Epoch 17, CIFAR-10 Batch 3:  loss  : 0.956556499004364 and Validation : 0.5622000098228455 \n",
      "Epoch 17, CIFAR-10 Batch 4:  loss  : 0.9880878329277039 and Validation : 0.5654000043869019 \n",
      "Epoch 17, CIFAR-10 Batch 5:  loss  : 1.246877908706665 and Validation : 0.5618000030517578 \n",
      "Epoch 18, CIFAR-10 Batch 1:  loss  : 1.189724326133728 and Validation : 0.5613999962806702 \n",
      "Epoch 18, CIFAR-10 Batch 2:  loss  : 1.044722557067871 and Validation : 0.5712000131607056 \n",
      "Epoch 18, CIFAR-10 Batch 3:  loss  : 0.9874998927116394 and Validation : 0.5667999982833862 \n",
      "Epoch 18, CIFAR-10 Batch 4:  loss  : 0.9782480001449585 and Validation : 0.5673999786376953 \n",
      "Epoch 18, CIFAR-10 Batch 5:  loss  : 1.2305855751037598 and Validation : 0.569599986076355 \n",
      "Epoch 19, CIFAR-10 Batch 1:  loss  : 1.1751846075057983 and Validation : 0.5681999921798706 \n",
      "Epoch 19, CIFAR-10 Batch 2:  loss  : 1.0236705541610718 and Validation : 0.5708000063896179 \n",
      "Epoch 19, CIFAR-10 Batch 3:  loss  : 0.962942898273468 and Validation : 0.5690000057220459 \n",
      "Epoch 19, CIFAR-10 Batch 4:  loss  : 0.9696930050849915 and Validation : 0.5766000151634216 \n",
      "Epoch 19, CIFAR-10 Batch 5:  loss  : 1.2428627014160156 and Validation : 0.5649999976158142 \n",
      "Epoch 20, CIFAR-10 Batch 1:  loss  : 1.149427890777588 and Validation : 0.5666000247001648 \n",
      "Epoch 20, CIFAR-10 Batch 2:  loss  : 1.000861406326294 and Validation : 0.5734000205993652 \n",
      "Epoch 20, CIFAR-10 Batch 3:  loss  : 0.9717696905136108 and Validation : 0.5684000253677368 \n",
      "Epoch 20, CIFAR-10 Batch 4:  loss  : 0.9449924230575562 and Validation : 0.5698000192642212 \n",
      "Epoch 20, CIFAR-10 Batch 5:  loss  : 1.2040210962295532 and Validation : 0.5766000151634216 \n",
      "Epoch 21, CIFAR-10 Batch 1:  loss  : 1.1217734813690186 and Validation : 0.5673999786376953 \n",
      "Epoch 21, CIFAR-10 Batch 2:  loss  : 1.0189529657363892 and Validation : 0.5726000070571899 \n",
      "Epoch 21, CIFAR-10 Batch 3:  loss  : 0.9389203190803528 and Validation : 0.5699999928474426 \n",
      "Epoch 21, CIFAR-10 Batch 4:  loss  : 0.9238265752792358 and Validation : 0.5667999982833862 \n",
      "Epoch 21, CIFAR-10 Batch 5:  loss  : 1.2045766115188599 and Validation : 0.5730000138282776 \n",
      "Epoch 22, CIFAR-10 Batch 1:  loss  : 1.1577870845794678 and Validation : 0.5727999806404114 \n",
      "Epoch 22, CIFAR-10 Batch 2:  loss  : 0.9813997149467468 and Validation : 0.574999988079071 \n",
      "Epoch 22, CIFAR-10 Batch 3:  loss  : 0.9506229162216187 and Validation : 0.5767999887466431 \n",
      "Epoch 22, CIFAR-10 Batch 4:  loss  : 0.8917333483695984 and Validation : 0.574999988079071 \n",
      "Epoch 22, CIFAR-10 Batch 5:  loss  : 1.193530797958374 and Validation : 0.5722000002861023 \n",
      "Epoch 23, CIFAR-10 Batch 1:  loss  : 1.0718814134597778 and Validation : 0.576200008392334 \n",
      "Epoch 23, CIFAR-10 Batch 2:  loss  : 0.9496294856071472 and Validation : 0.5774000287055969 \n",
      "Epoch 23, CIFAR-10 Batch 3:  loss  : 0.9238646626472473 and Validation : 0.5703999996185303 \n",
      "Epoch 23, CIFAR-10 Batch 4:  loss  : 0.8926590085029602 and Validation : 0.5794000029563904 \n",
      "Epoch 23, CIFAR-10 Batch 5:  loss  : 1.1766602993011475 and Validation : 0.5753999948501587 \n",
      "Epoch 24, CIFAR-10 Batch 1:  loss  : 1.0924056768417358 and Validation : 0.578000009059906 \n",
      "Epoch 24, CIFAR-10 Batch 2:  loss  : 0.9678575396537781 and Validation : 0.5809999704360962 \n",
      "Epoch 24, CIFAR-10 Batch 3:  loss  : 0.9510291814804077 and Validation : 0.5784000158309937 \n",
      "Epoch 24, CIFAR-10 Batch 4:  loss  : 0.8649214506149292 and Validation : 0.5803999900817871 \n",
      "Epoch 24, CIFAR-10 Batch 5:  loss  : 1.1372497081756592 and Validation : 0.5806000232696533 \n",
      "Epoch 25, CIFAR-10 Batch 1:  loss  : 1.100106120109558 and Validation : 0.5759999752044678 \n",
      "Epoch 25, CIFAR-10 Batch 2:  loss  : 0.9632043838500977 and Validation : 0.5748000144958496 \n",
      "Epoch 25, CIFAR-10 Batch 3:  loss  : 0.8807582855224609 and Validation : 0.5789999961853027 \n",
      "Epoch 25, CIFAR-10 Batch 4:  loss  : 0.8546029925346375 and Validation : 0.5830000042915344 \n",
      "Epoch 25, CIFAR-10 Batch 5:  loss  : 1.168334722518921 and Validation : 0.5795999765396118 \n",
      "Epoch 26, CIFAR-10 Batch 1:  loss  : 1.0537687540054321 and Validation : 0.5824000239372253 \n",
      "Epoch 26, CIFAR-10 Batch 2:  loss  : 0.9689914584159851 and Validation : 0.5770000219345093 \n",
      "Epoch 26, CIFAR-10 Batch 3:  loss  : 0.883392333984375 and Validation : 0.5839999914169312 \n",
      "Epoch 26, CIFAR-10 Batch 4:  loss  : 0.8687944412231445 and Validation : 0.5820000171661377 \n",
      "Epoch 26, CIFAR-10 Batch 5:  loss  : 1.1144217252731323 and Validation : 0.5799999833106995 \n",
      "Epoch 27, CIFAR-10 Batch 1:  loss  : 1.0314674377441406 and Validation : 0.5781999826431274 \n",
      "Epoch 27, CIFAR-10 Batch 2:  loss  : 0.9655541181564331 and Validation : 0.5834000110626221 \n",
      "Epoch 27, CIFAR-10 Batch 3:  loss  : 0.90282142162323 and Validation : 0.58160001039505 \n",
      "Epoch 27, CIFAR-10 Batch 4:  loss  : 0.8589223623275757 and Validation : 0.5817999839782715 \n",
      "Epoch 27, CIFAR-10 Batch 5:  loss  : 1.1300967931747437 and Validation : 0.5807999968528748 \n",
      "Epoch 28, CIFAR-10 Batch 1:  loss  : 1.0393794775009155 and Validation : 0.5799999833106995 \n",
      "Epoch 28, CIFAR-10 Batch 2:  loss  : 0.9905111193656921 and Validation : 0.5860000252723694 \n",
      "Epoch 28, CIFAR-10 Batch 3:  loss  : 0.8769122958183289 and Validation : 0.5830000042915344 \n",
      "Epoch 28, CIFAR-10 Batch 4:  loss  : 0.8608668446540833 and Validation : 0.5838000178337097 \n",
      "Epoch 28, CIFAR-10 Batch 5:  loss  : 1.1253331899642944 and Validation : 0.5824000239372253 \n",
      "Epoch 29, CIFAR-10 Batch 1:  loss  : 1.0227562189102173 and Validation : 0.5839999914169312 \n",
      "Epoch 29, CIFAR-10 Batch 2:  loss  : 1.012474775314331 and Validation : 0.5839999914169312 \n",
      "Epoch 29, CIFAR-10 Batch 3:  loss  : 0.8691006898880005 and Validation : 0.5848000049591064 \n",
      "Epoch 29, CIFAR-10 Batch 4:  loss  : 0.8584389686584473 and Validation : 0.5860000252723694 \n",
      "Epoch 29, CIFAR-10 Batch 5:  loss  : 1.1077916622161865 and Validation : 0.5863999724388123 \n",
      "Epoch 30, CIFAR-10 Batch 1:  loss  : 1.0262281894683838 and Validation : 0.5853999853134155 \n",
      "Epoch 30, CIFAR-10 Batch 2:  loss  : 0.9363489151000977 and Validation : 0.5884000062942505 \n",
      "Epoch 30, CIFAR-10 Batch 3:  loss  : 0.8588250875473022 and Validation : 0.5856000185012817 \n",
      "Epoch 30, CIFAR-10 Batch 4:  loss  : 0.8419069051742554 and Validation : 0.5879999995231628 \n",
      "Epoch 30, CIFAR-10 Batch 5:  loss  : 1.1117902994155884 and Validation : 0.5824000239372253 \n",
      "Epoch 31, CIFAR-10 Batch 1:  loss  : 0.9829503893852234 and Validation : 0.5881999731063843 \n",
      "Epoch 31, CIFAR-10 Batch 2:  loss  : 0.9333833456039429 and Validation : 0.5831999778747559 \n",
      "Epoch 31, CIFAR-10 Batch 3:  loss  : 0.8187370300292969 and Validation : 0.5843999981880188 \n",
      "Epoch 31, CIFAR-10 Batch 4:  loss  : 0.817173182964325 and Validation : 0.5906000137329102 \n",
      "Epoch 31, CIFAR-10 Batch 5:  loss  : 1.0812640190124512 and Validation : 0.5863999724388123 \n",
      "Epoch 32, CIFAR-10 Batch 1:  loss  : 0.9876238107681274 and Validation : 0.5824000239372253 \n",
      "Epoch 32, CIFAR-10 Batch 2:  loss  : 0.905559241771698 and Validation : 0.5871999859809875 \n",
      "Epoch 32, CIFAR-10 Batch 3:  loss  : 0.8193210363388062 and Validation : 0.5871999859809875 \n",
      "Epoch 32, CIFAR-10 Batch 4:  loss  : 0.796347439289093 and Validation : 0.5924000144004822 \n",
      "Epoch 32, CIFAR-10 Batch 5:  loss  : 1.0696207284927368 and Validation : 0.5902000069618225 \n",
      "Epoch 33, CIFAR-10 Batch 1:  loss  : 0.9703043103218079 and Validation : 0.5934000015258789 \n",
      "Epoch 33, CIFAR-10 Batch 2:  loss  : 0.8460291028022766 and Validation : 0.5929999947547913 \n",
      "Epoch 33, CIFAR-10 Batch 3:  loss  : 0.8102517127990723 and Validation : 0.5856000185012817 \n",
      "Epoch 33, CIFAR-10 Batch 4:  loss  : 0.8424164056777954 and Validation : 0.5852000117301941 \n",
      "Epoch 33, CIFAR-10 Batch 5:  loss  : 1.0381343364715576 and Validation : 0.5885999798774719 \n",
      "Epoch 34, CIFAR-10 Batch 1:  loss  : 0.9861629605293274 and Validation : 0.5821999907493591 \n",
      "Epoch 34, CIFAR-10 Batch 2:  loss  : 0.879706084728241 and Validation : 0.5914000272750854 \n",
      "Epoch 34, CIFAR-10 Batch 3:  loss  : 0.8181076049804688 and Validation : 0.5885999798774719 \n",
      "Epoch 34, CIFAR-10 Batch 4:  loss  : 0.7974126935005188 and Validation : 0.5885999798774719 \n",
      "Epoch 34, CIFAR-10 Batch 5:  loss  : 1.0447591543197632 and Validation : 0.5928000211715698 \n",
      "Epoch 35, CIFAR-10 Batch 1:  loss  : 0.9722583889961243 and Validation : 0.5896000266075134 \n",
      "Epoch 35, CIFAR-10 Batch 2:  loss  : 0.9012660980224609 and Validation : 0.5896000266075134 \n",
      "Epoch 35, CIFAR-10 Batch 3:  loss  : 0.8036781549453735 and Validation : 0.5878000259399414 \n",
      "Epoch 35, CIFAR-10 Batch 4:  loss  : 0.8006486892700195 and Validation : 0.5892000198364258 \n",
      "Epoch 35, CIFAR-10 Batch 5:  loss  : 1.040610671043396 and Validation : 0.5934000015258789 \n",
      "Epoch 36, CIFAR-10 Batch 1:  loss  : 0.976426899433136 and Validation : 0.5856000185012817 \n",
      "Epoch 36, CIFAR-10 Batch 2:  loss  : 0.8844987750053406 and Validation : 0.5902000069618225 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, CIFAR-10 Batch 3:  loss  : 0.8139234781265259 and Validation : 0.592199981212616 \n",
      "Epoch 36, CIFAR-10 Batch 4:  loss  : 0.7849535942077637 and Validation : 0.592199981212616 \n",
      "Epoch 36, CIFAR-10 Batch 5:  loss  : 1.0636985301971436 and Validation : 0.5860000252723694 \n",
      "Epoch 37, CIFAR-10 Batch 1:  loss  : 0.9786802530288696 and Validation : 0.5866000056266785 \n",
      "Epoch 37, CIFAR-10 Batch 2:  loss  : 0.9221776723861694 and Validation : 0.5971999764442444 \n",
      "Epoch 37, CIFAR-10 Batch 3:  loss  : 0.8189316987991333 and Validation : 0.5857999920845032 \n",
      "Epoch 37, CIFAR-10 Batch 4:  loss  : 0.7630001902580261 and Validation : 0.5950000286102295 \n",
      "Epoch 37, CIFAR-10 Batch 5:  loss  : 1.0065796375274658 and Validation : 0.5907999873161316 \n",
      "Epoch 38, CIFAR-10 Batch 1:  loss  : 0.9416079521179199 and Validation : 0.5902000069618225 \n",
      "Epoch 38, CIFAR-10 Batch 2:  loss  : 0.8689653277397156 and Validation : 0.5888000130653381 \n",
      "Epoch 38, CIFAR-10 Batch 3:  loss  : 0.8340129852294922 and Validation : 0.5906000137329102 \n",
      "Epoch 38, CIFAR-10 Batch 4:  loss  : 0.7731468081474304 and Validation : 0.5938000082969666 \n",
      "Epoch 38, CIFAR-10 Batch 5:  loss  : 1.0404659509658813 and Validation : 0.5899999737739563 \n",
      "Epoch 39, CIFAR-10 Batch 1:  loss  : 0.9255408048629761 and Validation : 0.5934000015258789 \n",
      "Epoch 39, CIFAR-10 Batch 2:  loss  : 0.8838533163070679 and Validation : 0.5943999886512756 \n",
      "Epoch 39, CIFAR-10 Batch 3:  loss  : 0.7866860628128052 and Validation : 0.5946000218391418 \n",
      "Epoch 39, CIFAR-10 Batch 4:  loss  : 0.7739003896713257 and Validation : 0.5961999893188477 \n",
      "Epoch 39, CIFAR-10 Batch 5:  loss  : 1.027144193649292 and Validation : 0.5917999744415283 \n",
      "Epoch 40, CIFAR-10 Batch 1:  loss  : 0.9342123866081238 and Validation : 0.5916000008583069 \n",
      "Epoch 40, CIFAR-10 Batch 2:  loss  : 0.8497241735458374 and Validation : 0.597000002861023 \n",
      "Epoch 40, CIFAR-10 Batch 3:  loss  : 0.7695785760879517 and Validation : 0.597000002861023 \n",
      "Epoch 40, CIFAR-10 Batch 4:  loss  : 0.7670723795890808 and Validation : 0.5964000225067139 \n",
      "Epoch 40, CIFAR-10 Batch 5:  loss  : 1.0207234621047974 and Validation : 0.5942000150680542 \n",
      "Epoch 41, CIFAR-10 Batch 1:  loss  : 0.9206142425537109 and Validation : 0.597000002861023 \n",
      "Epoch 41, CIFAR-10 Batch 2:  loss  : 0.8305484652519226 and Validation : 0.5985999703407288 \n",
      "Epoch 41, CIFAR-10 Batch 3:  loss  : 0.7783305048942566 and Validation : 0.5932000279426575 \n",
      "Epoch 41, CIFAR-10 Batch 4:  loss  : 0.7795276641845703 and Validation : 0.5902000069618225 \n",
      "Epoch 41, CIFAR-10 Batch 5:  loss  : 0.9946653246879578 and Validation : 0.597599983215332 \n",
      "Epoch 42, CIFAR-10 Batch 1:  loss  : 0.9352709054946899 and Validation : 0.5906000137329102 \n",
      "Epoch 42, CIFAR-10 Batch 2:  loss  : 0.8202247619628906 and Validation : 0.5947999954223633 \n",
      "Epoch 42, CIFAR-10 Batch 3:  loss  : 0.7925680875778198 and Validation : 0.5965999960899353 \n",
      "Epoch 42, CIFAR-10 Batch 4:  loss  : 0.7463635802268982 and Validation : 0.5978000164031982 \n",
      "Epoch 42, CIFAR-10 Batch 5:  loss  : 1.0229203701019287 and Validation : 0.5925999879837036 \n",
      "Epoch 43, CIFAR-10 Batch 1:  loss  : 0.9119617342948914 and Validation : 0.5953999757766724 \n",
      "Epoch 43, CIFAR-10 Batch 2:  loss  : 0.8673943281173706 and Validation : 0.599399983882904 \n",
      "Epoch 43, CIFAR-10 Batch 3:  loss  : 0.7633546590805054 and Validation : 0.5928000211715698 \n",
      "Epoch 43, CIFAR-10 Batch 4:  loss  : 0.7488705515861511 and Validation : 0.5950000286102295 \n",
      "Epoch 43, CIFAR-10 Batch 5:  loss  : 0.9962791204452515 and Validation : 0.5925999879837036 \n",
      "Epoch 44, CIFAR-10 Batch 1:  loss  : 0.9392199516296387 and Validation : 0.5831999778747559 \n",
      "Epoch 44, CIFAR-10 Batch 2:  loss  : 0.867489218711853 and Validation : 0.5996000170707703 \n",
      "Epoch 44, CIFAR-10 Batch 3:  loss  : 0.7677844762802124 and Validation : 0.5938000082969666 \n",
      "Epoch 44, CIFAR-10 Batch 4:  loss  : 0.7614639401435852 and Validation : 0.5983999967575073 \n",
      "Epoch 44, CIFAR-10 Batch 5:  loss  : 1.0079374313354492 and Validation : 0.6000000238418579 \n",
      "Epoch 45, CIFAR-10 Batch 1:  loss  : 0.9131643176078796 and Validation : 0.6021999716758728 \n",
      "Epoch 45, CIFAR-10 Batch 2:  loss  : 0.8210378885269165 and Validation : 0.5950000286102295 \n",
      "Epoch 45, CIFAR-10 Batch 3:  loss  : 0.7975941896438599 and Validation : 0.597599983215332 \n",
      "Epoch 45, CIFAR-10 Batch 4:  loss  : 0.7733395099639893 and Validation : 0.599399983882904 \n",
      "Epoch 45, CIFAR-10 Batch 5:  loss  : 0.9915910959243774 and Validation : 0.597000002861023 \n",
      "Epoch 46, CIFAR-10 Batch 1:  loss  : 0.9259846806526184 and Validation : 0.5953999757766724 \n",
      "Epoch 46, CIFAR-10 Batch 2:  loss  : 0.818618893623352 and Validation : 0.6011999845504761 \n",
      "Epoch 46, CIFAR-10 Batch 3:  loss  : 0.749178409576416 and Validation : 0.6047999858856201 \n",
      "Epoch 46, CIFAR-10 Batch 4:  loss  : 0.7692378759384155 and Validation : 0.6032000184059143 \n",
      "Epoch 46, CIFAR-10 Batch 5:  loss  : 0.9918570518493652 and Validation : 0.5978000164031982 \n",
      "Epoch 47, CIFAR-10 Batch 1:  loss  : 0.9091532826423645 and Validation : 0.5892000198364258 \n",
      "Epoch 47, CIFAR-10 Batch 2:  loss  : 0.8744677305221558 and Validation : 0.6014000177383423 \n",
      "Epoch 47, CIFAR-10 Batch 3:  loss  : 0.7409983277320862 and Validation : 0.6003999710083008 \n",
      "Epoch 47, CIFAR-10 Batch 4:  loss  : 0.7648012042045593 and Validation : 0.5996000170707703 \n",
      "Epoch 47, CIFAR-10 Batch 5:  loss  : 0.9717362523078918 and Validation : 0.6033999919891357 \n",
      "Epoch 48, CIFAR-10 Batch 1:  loss  : 0.9377971887588501 and Validation : 0.5893999934196472 \n",
      "Epoch 48, CIFAR-10 Batch 2:  loss  : 0.825901210308075 and Validation : 0.6057999730110168 \n",
      "Epoch 48, CIFAR-10 Batch 3:  loss  : 0.7506445646286011 and Validation : 0.604200005531311 \n",
      "Epoch 48, CIFAR-10 Batch 4:  loss  : 0.7373548150062561 and Validation : 0.6033999919891357 \n",
      "Epoch 48, CIFAR-10 Batch 5:  loss  : 0.973544716835022 and Validation : 0.604200005531311 \n",
      "Epoch 49, CIFAR-10 Batch 1:  loss  : 0.8558810949325562 and Validation : 0.6057999730110168 \n",
      "Epoch 49, CIFAR-10 Batch 2:  loss  : 0.8226802945137024 and Validation : 0.6039999723434448 \n",
      "Epoch 49, CIFAR-10 Batch 3:  loss  : 0.7562615275382996 and Validation : 0.605400025844574 \n",
      "Epoch 49, CIFAR-10 Batch 4:  loss  : 0.7355043292045593 and Validation : 0.6014000177383423 \n",
      "Epoch 49, CIFAR-10 Batch 5:  loss  : 0.9907209277153015 and Validation : 0.5943999886512756 \n",
      "Epoch 50, CIFAR-10 Batch 1:  loss  : 0.8632818460464478 and Validation : 0.5961999893188477 \n",
      "Epoch 50, CIFAR-10 Batch 2:  loss  : 0.8068678975105286 and Validation : 0.6029999852180481 \n",
      "Epoch 50, CIFAR-10 Batch 3:  loss  : 0.7512443661689758 and Validation : 0.6003999710083008 \n",
      "Epoch 50, CIFAR-10 Batch 4:  loss  : 0.7314087152481079 and Validation : 0.5982000231742859 \n",
      "Epoch 50, CIFAR-10 Batch 5:  loss  : 0.9748398065567017 and Validation : 0.6018000245094299 \n",
      "Epoch 51, CIFAR-10 Batch 1:  loss  : 0.8671028017997742 and Validation : 0.597599983215332 \n",
      "Epoch 51, CIFAR-10 Batch 2:  loss  : 0.8249999284744263 and Validation : 0.6074000000953674 \n",
      "Epoch 51, CIFAR-10 Batch 3:  loss  : 0.7385938763618469 and Validation : 0.6055999994277954 \n",
      "Epoch 51, CIFAR-10 Batch 4:  loss  : 0.7250237464904785 and Validation : 0.6093999743461609 \n",
      "Epoch 51, CIFAR-10 Batch 5:  loss  : 0.9688395261764526 and Validation : 0.6001999974250793 \n",
      "Epoch 52, CIFAR-10 Batch 1:  loss  : 0.8833196759223938 and Validation : 0.6010000109672546 \n",
      "Epoch 52, CIFAR-10 Batch 2:  loss  : 0.8287941217422485 and Validation : 0.6079999804496765 \n",
      "Epoch 52, CIFAR-10 Batch 3:  loss  : 0.7504885792732239 and Validation : 0.5989999771118164 \n",
      "Epoch 52, CIFAR-10 Batch 4:  loss  : 0.7206400632858276 and Validation : 0.604200005531311 \n",
      "Epoch 52, CIFAR-10 Batch 5:  loss  : 0.957789421081543 and Validation : 0.603600025177002 \n",
      "Epoch 53, CIFAR-10 Batch 1:  loss  : 0.8428767919540405 and Validation : 0.6060000061988831 \n",
      "Epoch 53, CIFAR-10 Batch 2:  loss  : 0.8305160403251648 and Validation : 0.6025999784469604 \n",
      "Epoch 53, CIFAR-10 Batch 3:  loss  : 0.7404471039772034 and Validation : 0.6007999777793884 \n",
      "Epoch 53, CIFAR-10 Batch 4:  loss  : 0.7267193794250488 and Validation : 0.6064000129699707 \n",
      "Epoch 53, CIFAR-10 Batch 5:  loss  : 0.9473773837089539 and Validation : 0.6011999845504761 \n",
      "Epoch 54, CIFAR-10 Batch 1:  loss  : 0.8319036364555359 and Validation : 0.6068000197410583 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, CIFAR-10 Batch 2:  loss  : 0.8364814519882202 and Validation : 0.6001999974250793 \n",
      "Epoch 54, CIFAR-10 Batch 3:  loss  : 0.7228552103042603 and Validation : 0.6018000245094299 \n",
      "Epoch 54, CIFAR-10 Batch 4:  loss  : 0.7240073680877686 and Validation : 0.6055999994277954 \n",
      "Epoch 54, CIFAR-10 Batch 5:  loss  : 0.9400882720947266 and Validation : 0.6057999730110168 \n",
      "Epoch 55, CIFAR-10 Batch 1:  loss  : 0.8481447100639343 and Validation : 0.6003999710083008 \n",
      "Epoch 55, CIFAR-10 Batch 2:  loss  : 0.8304986953735352 and Validation : 0.6092000007629395 \n",
      "Epoch 55, CIFAR-10 Batch 3:  loss  : 0.727782130241394 and Validation : 0.6068000197410583 \n",
      "Epoch 55, CIFAR-10 Batch 4:  loss  : 0.7149187922477722 and Validation : 0.607200026512146 \n",
      "Epoch 55, CIFAR-10 Batch 5:  loss  : 0.9229529500007629 and Validation : 0.602400004863739 \n",
      "Epoch 56, CIFAR-10 Batch 1:  loss  : 0.8355976343154907 and Validation : 0.6007999777793884 \n",
      "Epoch 56, CIFAR-10 Batch 2:  loss  : 0.8283394575119019 and Validation : 0.605400025844574 \n",
      "Epoch 56, CIFAR-10 Batch 3:  loss  : 0.6966129541397095 and Validation : 0.605400025844574 \n",
      "Epoch 56, CIFAR-10 Batch 4:  loss  : 0.7235447764396667 and Validation : 0.6003999710083008 \n",
      "Epoch 56, CIFAR-10 Batch 5:  loss  : 0.9339044690132141 and Validation : 0.605400025844574 \n",
      "Epoch 57, CIFAR-10 Batch 1:  loss  : 0.8288692235946655 and Validation : 0.6075999736785889 \n",
      "Epoch 57, CIFAR-10 Batch 2:  loss  : 0.8263843655586243 and Validation : 0.6028000116348267 \n",
      "Epoch 57, CIFAR-10 Batch 3:  loss  : 0.685566246509552 and Validation : 0.6100000143051147 \n",
      "Epoch 57, CIFAR-10 Batch 4:  loss  : 0.706036388874054 and Validation : 0.6075999736785889 \n",
      "Epoch 57, CIFAR-10 Batch 5:  loss  : 0.9294630885124207 and Validation : 0.6029999852180481 \n",
      "Epoch 58, CIFAR-10 Batch 1:  loss  : 0.8214665651321411 and Validation : 0.6001999974250793 \n",
      "Epoch 58, CIFAR-10 Batch 2:  loss  : 0.7937385439872742 and Validation : 0.6100000143051147 \n",
      "Epoch 58, CIFAR-10 Batch 3:  loss  : 0.7410846948623657 and Validation : 0.6029999852180481 \n",
      "Epoch 58, CIFAR-10 Batch 4:  loss  : 0.7262386083602905 and Validation : 0.6086000204086304 \n",
      "Epoch 58, CIFAR-10 Batch 5:  loss  : 0.9182519912719727 and Validation : 0.6057999730110168 \n",
      "Epoch 59, CIFAR-10 Batch 1:  loss  : 0.8241389393806458 and Validation : 0.6078000068664551 \n",
      "Epoch 59, CIFAR-10 Batch 2:  loss  : 0.7930724620819092 and Validation : 0.61080002784729 \n",
      "Epoch 59, CIFAR-10 Batch 3:  loss  : 0.6966809034347534 and Validation : 0.605400025844574 \n",
      "Epoch 59, CIFAR-10 Batch 4:  loss  : 0.7121524810791016 and Validation : 0.6079999804496765 \n",
      "Epoch 59, CIFAR-10 Batch 5:  loss  : 0.9246891736984253 and Validation : 0.6061999797821045 \n",
      "Epoch 60, CIFAR-10 Batch 1:  loss  : 0.8269426226615906 and Validation : 0.6021999716758728 \n",
      "Epoch 60, CIFAR-10 Batch 2:  loss  : 0.797764778137207 and Validation : 0.6051999926567078 \n",
      "Epoch 60, CIFAR-10 Batch 3:  loss  : 0.7096265554428101 and Validation : 0.6064000129699707 \n",
      "Epoch 60, CIFAR-10 Batch 4:  loss  : 0.7102922797203064 and Validation : 0.6065999865531921 \n",
      "Epoch 60, CIFAR-10 Batch 5:  loss  : 0.9216710925102234 and Validation : 0.6015999913215637 \n",
      "Epoch 61, CIFAR-10 Batch 1:  loss  : 0.8298481106758118 and Validation : 0.5978000164031982 \n",
      "Epoch 61, CIFAR-10 Batch 2:  loss  : 0.7862248420715332 and Validation : 0.6083999872207642 \n",
      "Epoch 61, CIFAR-10 Batch 3:  loss  : 0.689125657081604 and Validation : 0.6092000007629395 \n",
      "Epoch 61, CIFAR-10 Batch 4:  loss  : 0.7202777862548828 and Validation : 0.6097999811172485 \n",
      "Epoch 61, CIFAR-10 Batch 5:  loss  : 0.9240198135375977 and Validation : 0.607200026512146 \n",
      "Epoch 62, CIFAR-10 Batch 1:  loss  : 0.8427575826644897 and Validation : 0.6055999994277954 \n",
      "Epoch 62, CIFAR-10 Batch 2:  loss  : 0.7561293840408325 and Validation : 0.6172000169754028 \n",
      "Epoch 62, CIFAR-10 Batch 3:  loss  : 0.7059134244918823 and Validation : 0.6129999756813049 \n",
      "Epoch 62, CIFAR-10 Batch 4:  loss  : 0.6882869005203247 and Validation : 0.6114000082015991 \n",
      "Epoch 62, CIFAR-10 Batch 5:  loss  : 0.9290167689323425 and Validation : 0.6000000238418579 \n",
      "Epoch 63, CIFAR-10 Batch 1:  loss  : 0.8197119832038879 and Validation : 0.5996000170707703 \n",
      "Epoch 63, CIFAR-10 Batch 2:  loss  : 0.7664719820022583 and Validation : 0.6114000082015991 \n",
      "Epoch 63, CIFAR-10 Batch 3:  loss  : 0.7078003883361816 and Validation : 0.6069999933242798 \n",
      "Epoch 63, CIFAR-10 Batch 4:  loss  : 0.7167037725448608 and Validation : 0.6100000143051147 \n",
      "Epoch 63, CIFAR-10 Batch 5:  loss  : 0.9196969866752625 and Validation : 0.605400025844574 \n",
      "Epoch 64, CIFAR-10 Batch 1:  loss  : 0.7999690175056458 and Validation : 0.6068000197410583 \n",
      "Epoch 64, CIFAR-10 Batch 2:  loss  : 0.7546265125274658 and Validation : 0.6097999811172485 \n",
      "Epoch 64, CIFAR-10 Batch 3:  loss  : 0.6918960809707642 and Validation : 0.6092000007629395 \n",
      "Epoch 64, CIFAR-10 Batch 4:  loss  : 0.6918610334396362 and Validation : 0.6132000088691711 \n",
      "Epoch 64, CIFAR-10 Batch 5:  loss  : 0.9257408380508423 and Validation : 0.6074000000953674 \n",
      "Epoch 65, CIFAR-10 Batch 1:  loss  : 0.808604896068573 and Validation : 0.597000002861023 \n",
      "Epoch 65, CIFAR-10 Batch 2:  loss  : 0.7354000210762024 and Validation : 0.6137999892234802 \n",
      "Epoch 65, CIFAR-10 Batch 3:  loss  : 0.6647385358810425 and Validation : 0.6083999872207642 \n",
      "Epoch 65, CIFAR-10 Batch 4:  loss  : 0.6831542253494263 and Validation : 0.6093999743461609 \n",
      "Epoch 65, CIFAR-10 Batch 5:  loss  : 0.9213360548019409 and Validation : 0.6122000217437744 \n",
      "Epoch 66, CIFAR-10 Batch 1:  loss  : 0.7872502207756042 and Validation : 0.6097999811172485 \n",
      "Epoch 66, CIFAR-10 Batch 2:  loss  : 0.7608868479728699 and Validation : 0.6133999824523926 \n",
      "Epoch 66, CIFAR-10 Batch 3:  loss  : 0.7249995470046997 and Validation : 0.61080002784729 \n",
      "Epoch 66, CIFAR-10 Batch 4:  loss  : 0.6879106163978577 and Validation : 0.6118000149726868 \n",
      "Epoch 66, CIFAR-10 Batch 5:  loss  : 0.8973226547241211 and Validation : 0.6118000149726868 \n",
      "Epoch 67, CIFAR-10 Batch 1:  loss  : 0.8245241045951843 and Validation : 0.6018000245094299 \n",
      "Epoch 67, CIFAR-10 Batch 2:  loss  : 0.7566490173339844 and Validation : 0.6137999892234802 \n",
      "Epoch 67, CIFAR-10 Batch 3:  loss  : 0.6782911419868469 and Validation : 0.6137999892234802 \n",
      "Epoch 67, CIFAR-10 Batch 4:  loss  : 0.6828837394714355 and Validation : 0.6118000149726868 \n",
      "Epoch 67, CIFAR-10 Batch 5:  loss  : 0.903942883014679 and Validation : 0.6111999750137329 \n",
      "Epoch 68, CIFAR-10 Batch 1:  loss  : 0.815808892250061 and Validation : 0.607200026512146 \n",
      "Epoch 68, CIFAR-10 Batch 2:  loss  : 0.7445328831672668 and Validation : 0.6126000285148621 \n",
      "Epoch 68, CIFAR-10 Batch 3:  loss  : 0.6889573931694031 and Validation : 0.6087999939918518 \n",
      "Epoch 68, CIFAR-10 Batch 4:  loss  : 0.6851892471313477 and Validation : 0.6105999946594238 \n",
      "Epoch 68, CIFAR-10 Batch 5:  loss  : 0.9015396237373352 and Validation : 0.6087999939918518 \n",
      "Epoch 69, CIFAR-10 Batch 1:  loss  : 0.7937777042388916 and Validation : 0.5974000096321106 \n",
      "Epoch 69, CIFAR-10 Batch 2:  loss  : 0.765734076499939 and Validation : 0.6123999953269958 \n",
      "Epoch 69, CIFAR-10 Batch 3:  loss  : 0.6689314246177673 and Validation : 0.6136000156402588 \n",
      "Epoch 69, CIFAR-10 Batch 4:  loss  : 0.679535448551178 and Validation : 0.6110000014305115 \n",
      "Epoch 69, CIFAR-10 Batch 5:  loss  : 0.9104528427124023 and Validation : 0.6083999872207642 \n",
      "Epoch 70, CIFAR-10 Batch 1:  loss  : 0.7671247720718384 and Validation : 0.6079999804496765 \n",
      "Epoch 70, CIFAR-10 Batch 2:  loss  : 0.7180473804473877 and Validation : 0.6115999817848206 \n",
      "Epoch 70, CIFAR-10 Batch 3:  loss  : 0.6826673746109009 and Validation : 0.6115999817848206 \n",
      "Epoch 70, CIFAR-10 Batch 4:  loss  : 0.6910024881362915 and Validation : 0.6115999817848206 \n",
      "Epoch 70, CIFAR-10 Batch 5:  loss  : 0.9028016924858093 and Validation : 0.6064000129699707 \n",
      "Epoch 71, CIFAR-10 Batch 1:  loss  : 0.7948135137557983 and Validation : 0.61080002784729 \n",
      "Epoch 71, CIFAR-10 Batch 2:  loss  : 0.7083252668380737 and Validation : 0.6144000291824341 \n",
      "Epoch 71, CIFAR-10 Batch 3:  loss  : 0.704889178276062 and Validation : 0.6111999750137329 \n",
      "Epoch 71, CIFAR-10 Batch 4:  loss  : 0.6760819554328918 and Validation : 0.6150000095367432 \n",
      "Epoch 71, CIFAR-10 Batch 5:  loss  : 0.9032073020935059 and Validation : 0.5983999967575073 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, CIFAR-10 Batch 1:  loss  : 0.8110456466674805 and Validation : 0.604200005531311 \n",
      "Epoch 72, CIFAR-10 Batch 2:  loss  : 0.7084186673164368 and Validation : 0.6179999709129333 \n",
      "Epoch 72, CIFAR-10 Batch 3:  loss  : 0.6974602937698364 and Validation : 0.6104000210762024 \n",
      "Epoch 72, CIFAR-10 Batch 4:  loss  : 0.6705177426338196 and Validation : 0.6114000082015991 \n",
      "Epoch 72, CIFAR-10 Batch 5:  loss  : 0.8568056225776672 and Validation : 0.6118000149726868 \n",
      "Epoch 73, CIFAR-10 Batch 1:  loss  : 0.7747146487236023 and Validation : 0.6105999946594238 \n",
      "Epoch 73, CIFAR-10 Batch 2:  loss  : 0.7371647357940674 and Validation : 0.6082000136375427 \n",
      "Epoch 73, CIFAR-10 Batch 3:  loss  : 0.7001780271530151 and Validation : 0.6074000000953674 \n",
      "Epoch 73, CIFAR-10 Batch 4:  loss  : 0.6687029004096985 and Validation : 0.6100000143051147 \n",
      "Epoch 73, CIFAR-10 Batch 5:  loss  : 0.8734859228134155 and Validation : 0.6083999872207642 \n",
      "Epoch 74, CIFAR-10 Batch 1:  loss  : 0.7720346450805664 and Validation : 0.6168000102043152 \n",
      "Epoch 74, CIFAR-10 Batch 2:  loss  : 0.7007281184196472 and Validation : 0.6158000230789185 \n",
      "Epoch 74, CIFAR-10 Batch 3:  loss  : 0.6851891279220581 and Validation : 0.6122000217437744 \n",
      "Epoch 74, CIFAR-10 Batch 4:  loss  : 0.6805657744407654 and Validation : 0.6141999959945679 \n",
      "Epoch 74, CIFAR-10 Batch 5:  loss  : 0.9024728536605835 and Validation : 0.6014000177383423 \n",
      "Epoch 75, CIFAR-10 Batch 1:  loss  : 0.7832680940628052 and Validation : 0.6092000007629395 \n",
      "Epoch 75, CIFAR-10 Batch 2:  loss  : 0.7052701115608215 and Validation : 0.6140000224113464 \n",
      "Epoch 75, CIFAR-10 Batch 3:  loss  : 0.6423044204711914 and Validation : 0.6100000143051147 \n",
      "Epoch 75, CIFAR-10 Batch 4:  loss  : 0.6856015920639038 and Validation : 0.6083999872207642 \n",
      "Epoch 75, CIFAR-10 Batch 5:  loss  : 0.8887075185775757 and Validation : 0.6093999743461609 \n",
      "Epoch 76, CIFAR-10 Batch 1:  loss  : 0.7666106224060059 and Validation : 0.614799976348877 \n",
      "Epoch 76, CIFAR-10 Batch 2:  loss  : 0.7163792848587036 and Validation : 0.6161999702453613 \n",
      "Epoch 76, CIFAR-10 Batch 3:  loss  : 0.6583185195922852 and Validation : 0.6101999878883362 \n",
      "Epoch 76, CIFAR-10 Batch 4:  loss  : 0.6846729516983032 and Validation : 0.6105999946594238 \n",
      "Epoch 76, CIFAR-10 Batch 5:  loss  : 0.864808201789856 and Validation : 0.6082000136375427 \n",
      "Epoch 77, CIFAR-10 Batch 1:  loss  : 0.7804501056671143 and Validation : 0.6126000285148621 \n",
      "Epoch 77, CIFAR-10 Batch 2:  loss  : 0.702779233455658 and Validation : 0.6172000169754028 \n",
      "Epoch 77, CIFAR-10 Batch 3:  loss  : 0.6606518030166626 and Validation : 0.61080002784729 \n",
      "Epoch 77, CIFAR-10 Batch 4:  loss  : 0.6596285700798035 and Validation : 0.6129999756813049 \n",
      "Epoch 77, CIFAR-10 Batch 5:  loss  : 0.8893383145332336 and Validation : 0.604200005531311 \n",
      "Epoch 78, CIFAR-10 Batch 1:  loss  : 0.7620290517807007 and Validation : 0.6104000210762024 \n",
      "Epoch 78, CIFAR-10 Batch 2:  loss  : 0.6999539136886597 and Validation : 0.6146000027656555 \n",
      "Epoch 78, CIFAR-10 Batch 3:  loss  : 0.6547296643257141 and Validation : 0.6129999756813049 \n",
      "Epoch 78, CIFAR-10 Batch 4:  loss  : 0.6488200426101685 and Validation : 0.6190000176429749 \n",
      "Epoch 78, CIFAR-10 Batch 5:  loss  : 0.8519716262817383 and Validation : 0.6136000156402588 \n",
      "Epoch 79, CIFAR-10 Batch 1:  loss  : 0.7899671196937561 and Validation : 0.6037999987602234 \n",
      "Epoch 79, CIFAR-10 Batch 2:  loss  : 0.6934119462966919 and Validation : 0.6176000237464905 \n",
      "Epoch 79, CIFAR-10 Batch 3:  loss  : 0.6744799017906189 and Validation : 0.614799976348877 \n",
      "Epoch 79, CIFAR-10 Batch 4:  loss  : 0.6734219193458557 and Validation : 0.61080002784729 \n",
      "Epoch 79, CIFAR-10 Batch 5:  loss  : 0.8769605755805969 and Validation : 0.6119999885559082 \n",
      "Epoch 80, CIFAR-10 Batch 1:  loss  : 0.771770715713501 and Validation : 0.6078000068664551 \n",
      "Epoch 80, CIFAR-10 Batch 2:  loss  : 0.700697124004364 and Validation : 0.6150000095367432 \n",
      "Epoch 80, CIFAR-10 Batch 3:  loss  : 0.687500536441803 and Validation : 0.6151999831199646 \n",
      "Epoch 80, CIFAR-10 Batch 4:  loss  : 0.6446019411087036 and Validation : 0.6182000041007996 \n",
      "Epoch 80, CIFAR-10 Batch 5:  loss  : 0.8747516870498657 and Validation : 0.6093999743461609 \n",
      "Epoch 81, CIFAR-10 Batch 1:  loss  : 0.7535297274589539 and Validation : 0.6096000075340271 \n",
      "Epoch 81, CIFAR-10 Batch 2:  loss  : 0.7030519247055054 and Validation : 0.6122000217437744 \n",
      "Epoch 81, CIFAR-10 Batch 3:  loss  : 0.6579967737197876 and Validation : 0.6144000291824341 \n",
      "Epoch 81, CIFAR-10 Batch 4:  loss  : 0.6583884954452515 and Validation : 0.6241999864578247 \n",
      "Epoch 81, CIFAR-10 Batch 5:  loss  : 0.8707844018936157 and Validation : 0.6079999804496765 \n",
      "Epoch 82, CIFAR-10 Batch 1:  loss  : 0.7462515234947205 and Validation : 0.6119999885559082 \n",
      "Epoch 82, CIFAR-10 Batch 2:  loss  : 0.6775338649749756 and Validation : 0.6208000183105469 \n",
      "Epoch 82, CIFAR-10 Batch 3:  loss  : 0.6418879628181458 and Validation : 0.6164000034332275 \n",
      "Epoch 82, CIFAR-10 Batch 4:  loss  : 0.6533094048500061 and Validation : 0.6200000047683716 \n",
      "Epoch 82, CIFAR-10 Batch 5:  loss  : 0.8690156936645508 and Validation : 0.6115999817848206 \n",
      "Epoch 83, CIFAR-10 Batch 1:  loss  : 0.7305817008018494 and Validation : 0.6037999987602234 \n",
      "Epoch 83, CIFAR-10 Batch 2:  loss  : 0.6914767026901245 and Validation : 0.6209999918937683 \n",
      "Epoch 83, CIFAR-10 Batch 3:  loss  : 0.6425172090530396 and Validation : 0.6126000285148621 \n",
      "Epoch 83, CIFAR-10 Batch 4:  loss  : 0.642154335975647 and Validation : 0.6222000122070312 \n",
      "Epoch 83, CIFAR-10 Batch 5:  loss  : 0.8857662081718445 and Validation : 0.6159999966621399 \n",
      "Epoch 84, CIFAR-10 Batch 1:  loss  : 0.7901378870010376 and Validation : 0.604200005531311 \n",
      "Epoch 84, CIFAR-10 Batch 2:  loss  : 0.6978520154953003 and Validation : 0.6182000041007996 \n",
      "Epoch 84, CIFAR-10 Batch 3:  loss  : 0.6461777687072754 and Validation : 0.6129999756813049 \n",
      "Epoch 84, CIFAR-10 Batch 4:  loss  : 0.670946478843689 and Validation : 0.6154000163078308 \n",
      "Epoch 84, CIFAR-10 Batch 5:  loss  : 0.8517285585403442 and Validation : 0.6177999973297119 \n",
      "Epoch 85, CIFAR-10 Batch 1:  loss  : 0.7507532238960266 and Validation : 0.6133999824523926 \n",
      "Epoch 85, CIFAR-10 Batch 2:  loss  : 0.6908150315284729 and Validation : 0.6137999892234802 \n",
      "Epoch 85, CIFAR-10 Batch 3:  loss  : 0.66139155626297 and Validation : 0.6172000169754028 \n",
      "Epoch 85, CIFAR-10 Batch 4:  loss  : 0.6265205144882202 and Validation : 0.6172000169754028 \n",
      "Epoch 85, CIFAR-10 Batch 5:  loss  : 0.876043975353241 and Validation : 0.6092000007629395 \n",
      "Epoch 86, CIFAR-10 Batch 1:  loss  : 0.7551348805427551 and Validation : 0.6060000061988831 \n",
      "Epoch 86, CIFAR-10 Batch 2:  loss  : 0.7222931385040283 and Validation : 0.6191999912261963 \n",
      "Epoch 86, CIFAR-10 Batch 3:  loss  : 0.6415907740592957 and Validation : 0.6155999898910522 \n",
      "Epoch 86, CIFAR-10 Batch 4:  loss  : 0.691516637802124 and Validation : 0.6110000014305115 \n",
      "Epoch 86, CIFAR-10 Batch 5:  loss  : 0.8451327085494995 and Validation : 0.6129999756813049 \n",
      "Epoch 87, CIFAR-10 Batch 1:  loss  : 0.7526435852050781 and Validation : 0.6100000143051147 \n",
      "Epoch 87, CIFAR-10 Batch 2:  loss  : 0.6933481097221375 and Validation : 0.6204000115394592 \n",
      "Epoch 87, CIFAR-10 Batch 3:  loss  : 0.6234476566314697 and Validation : 0.6105999946594238 \n",
      "Epoch 87, CIFAR-10 Batch 4:  loss  : 0.659359335899353 and Validation : 0.6164000034332275 \n",
      "Epoch 87, CIFAR-10 Batch 5:  loss  : 0.8485313653945923 and Validation : 0.6161999702453613 \n",
      "Epoch 88, CIFAR-10 Batch 1:  loss  : 0.7393539547920227 and Validation : 0.6118000149726868 \n",
      "Epoch 88, CIFAR-10 Batch 2:  loss  : 0.6934916377067566 and Validation : 0.6151999831199646 \n",
      "Epoch 88, CIFAR-10 Batch 3:  loss  : 0.6199066638946533 and Validation : 0.6093999743461609 \n",
      "Epoch 88, CIFAR-10 Batch 4:  loss  : 0.6450715065002441 and Validation : 0.6176000237464905 \n",
      "Epoch 88, CIFAR-10 Batch 5:  loss  : 0.8662430047988892 and Validation : 0.6158000230789185 \n",
      "Epoch 89, CIFAR-10 Batch 1:  loss  : 0.7314118146896362 and Validation : 0.6128000020980835 \n",
      "Epoch 89, CIFAR-10 Batch 2:  loss  : 0.6566775441169739 and Validation : 0.6223999857902527 \n",
      "Epoch 89, CIFAR-10 Batch 3:  loss  : 0.6339802742004395 and Validation : 0.6187999844551086 \n",
      "Epoch 89, CIFAR-10 Batch 4:  loss  : 0.6532647013664246 and Validation : 0.6208000183105469 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, CIFAR-10 Batch 5:  loss  : 0.8484571576118469 and Validation : 0.6064000129699707 \n",
      "Epoch 90, CIFAR-10 Batch 1:  loss  : 0.7499750256538391 and Validation : 0.6082000136375427 \n",
      "Epoch 90, CIFAR-10 Batch 2:  loss  : 0.6815458536148071 and Validation : 0.6173999905586243 \n",
      "Epoch 90, CIFAR-10 Batch 3:  loss  : 0.634812593460083 and Validation : 0.6144000291824341 \n",
      "Epoch 90, CIFAR-10 Batch 4:  loss  : 0.6502774953842163 and Validation : 0.6144000291824341 \n",
      "Epoch 90, CIFAR-10 Batch 5:  loss  : 0.8260358572006226 and Validation : 0.6182000041007996 \n",
      "Epoch 91, CIFAR-10 Batch 1:  loss  : 0.7349469065666199 and Validation : 0.6151999831199646 \n",
      "Epoch 91, CIFAR-10 Batch 2:  loss  : 0.6462174654006958 and Validation : 0.621999979019165 \n",
      "Epoch 91, CIFAR-10 Batch 3:  loss  : 0.6347764134407043 and Validation : 0.6182000041007996 \n",
      "Epoch 91, CIFAR-10 Batch 4:  loss  : 0.632917582988739 and Validation : 0.6169999837875366 \n",
      "Epoch 91, CIFAR-10 Batch 5:  loss  : 0.8508607745170593 and Validation : 0.6151999831199646 \n",
      "Epoch 92, CIFAR-10 Batch 1:  loss  : 0.7189432978630066 and Validation : 0.6191999912261963 \n",
      "Epoch 92, CIFAR-10 Batch 2:  loss  : 0.6794706583023071 and Validation : 0.6218000054359436 \n",
      "Epoch 92, CIFAR-10 Batch 3:  loss  : 0.6256097555160522 and Validation : 0.6154000163078308 \n",
      "Epoch 92, CIFAR-10 Batch 4:  loss  : 0.6576301455497742 and Validation : 0.6114000082015991 \n",
      "Epoch 92, CIFAR-10 Batch 5:  loss  : 0.8276954889297485 and Validation : 0.6096000075340271 \n",
      "Epoch 93, CIFAR-10 Batch 1:  loss  : 0.7207998633384705 and Validation : 0.6144000291824341 \n",
      "Epoch 93, CIFAR-10 Batch 2:  loss  : 0.6394971609115601 and Validation : 0.6236000061035156 \n",
      "Epoch 93, CIFAR-10 Batch 3:  loss  : 0.5971613526344299 and Validation : 0.6100000143051147 \n",
      "Epoch 93, CIFAR-10 Batch 4:  loss  : 0.6406486630439758 and Validation : 0.6146000027656555 \n",
      "Epoch 93, CIFAR-10 Batch 5:  loss  : 0.8194987177848816 and Validation : 0.6151999831199646 \n",
      "Epoch 94, CIFAR-10 Batch 1:  loss  : 0.6979106068611145 and Validation : 0.6169999837875366 \n",
      "Epoch 94, CIFAR-10 Batch 2:  loss  : 0.6849595308303833 and Validation : 0.6194000244140625 \n",
      "Epoch 94, CIFAR-10 Batch 3:  loss  : 0.6196595430374146 and Validation : 0.6154000163078308 \n",
      "Epoch 94, CIFAR-10 Batch 4:  loss  : 0.6446300745010376 and Validation : 0.6200000047683716 \n",
      "Epoch 94, CIFAR-10 Batch 5:  loss  : 0.8634575009346008 and Validation : 0.6129999756813049 \n",
      "Epoch 95, CIFAR-10 Batch 1:  loss  : 0.7298522591590881 and Validation : 0.6092000007629395 \n",
      "Epoch 95, CIFAR-10 Batch 2:  loss  : 0.6591758728027344 and Validation : 0.6172000169754028 \n",
      "Epoch 95, CIFAR-10 Batch 3:  loss  : 0.6467570066452026 and Validation : 0.6159999966621399 \n",
      "Epoch 95, CIFAR-10 Batch 4:  loss  : 0.6492654085159302 and Validation : 0.6168000102043152 \n",
      "Epoch 95, CIFAR-10 Batch 5:  loss  : 0.8458721041679382 and Validation : 0.6092000007629395 \n",
      "Epoch 96, CIFAR-10 Batch 1:  loss  : 0.7228434681892395 and Validation : 0.6154000163078308 \n",
      "Epoch 96, CIFAR-10 Batch 2:  loss  : 0.6479344367980957 and Validation : 0.6237999796867371 \n",
      "Epoch 96, CIFAR-10 Batch 3:  loss  : 0.612017035484314 and Validation : 0.6190000176429749 \n",
      "Epoch 96, CIFAR-10 Batch 4:  loss  : 0.640626072883606 and Validation : 0.6205999851226807 \n",
      "Epoch 96, CIFAR-10 Batch 5:  loss  : 0.8405880928039551 and Validation : 0.6115999817848206 \n",
      "Epoch 97, CIFAR-10 Batch 1:  loss  : 0.7151136994361877 and Validation : 0.6176000237464905 \n",
      "Epoch 97, CIFAR-10 Batch 2:  loss  : 0.6605392694473267 and Validation : 0.6226000189781189 \n",
      "Epoch 97, CIFAR-10 Batch 3:  loss  : 0.627989649772644 and Validation : 0.6233999729156494 \n",
      "Epoch 97, CIFAR-10 Batch 4:  loss  : 0.6338971257209778 and Validation : 0.6191999912261963 \n",
      "Epoch 97, CIFAR-10 Batch 5:  loss  : 0.8323028683662415 and Validation : 0.6123999953269958 \n",
      "Epoch 98, CIFAR-10 Batch 1:  loss  : 0.6891393661499023 and Validation : 0.6151999831199646 \n",
      "Epoch 98, CIFAR-10 Batch 2:  loss  : 0.6483999490737915 and Validation : 0.6223999857902527 \n",
      "Epoch 98, CIFAR-10 Batch 3:  loss  : 0.5850273370742798 and Validation : 0.6230000257492065 \n",
      "Epoch 98, CIFAR-10 Batch 4:  loss  : 0.6118442416191101 and Validation : 0.6266000270843506 \n",
      "Epoch 98, CIFAR-10 Batch 5:  loss  : 0.8183057904243469 and Validation : 0.6097999811172485 \n",
      "Epoch 99, CIFAR-10 Batch 1:  loss  : 0.6988157629966736 and Validation : 0.6215999722480774 \n",
      "Epoch 99, CIFAR-10 Batch 2:  loss  : 0.6513875126838684 and Validation : 0.6236000061035156 \n",
      "Epoch 99, CIFAR-10 Batch 3:  loss  : 0.61555016040802 and Validation : 0.6222000122070312 \n",
      "Epoch 99, CIFAR-10 Batch 4:  loss  : 0.625381350517273 and Validation : 0.6212000250816345 \n",
      "Epoch 99, CIFAR-10 Batch 5:  loss  : 0.8325620889663696 and Validation : 0.61080002784729 \n",
      "Epoch 100, CIFAR-10 Batch 1:  loss  : 0.7071229815483093 and Validation : 0.6222000122070312 \n",
      "Epoch 100, CIFAR-10 Batch 2:  loss  : 0.6515137553215027 and Validation : 0.6186000108718872 \n",
      "Epoch 100, CIFAR-10 Batch 3:  loss  : 0.5967338681221008 and Validation : 0.6172000169754028 \n",
      "Epoch 100, CIFAR-10 Batch 4:  loss  : 0.6267822980880737 and Validation : 0.6197999715805054 \n",
      "Epoch 100, CIFAR-10 Batch 5:  loss  : 0.8537472486495972 and Validation : 0.6037999987602234 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6042325949367089\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23804366b70>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
